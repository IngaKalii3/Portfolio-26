# The Annual Reckoning: AI Predictions vs. Reality (and a Fresh Batch for the Years Ahead)

Every year I run the same experiment: I collect a pile of AI predictions, then I come back later and check what actually happened. Not the fuzzy stuff. Not the "AGI is near" vibes. I mean specific, checkable claims—especially short-horizon ones—because they fail loudly and quickly when they're wrong.

This year's results rhyme with previous years: predictions about 2025 made in 2023–2025 mostly overshot actual capability progress. That pattern is distorted by selection effects (people who bother making near-term calls are often the ones most convinced AI will be impressive soon), but even with that caveat, the misses pile up.

There's also a slow-motion semantic collapse happening: "AGI" keeps getting less useful as a term. The concept isn't necessarily meaningless—but it's increasingly elastic. When the target stretches, evaluation becomes theater. The only antidote is operationalization: what exactly should the system do, in what setting, with what reliability, under what constraints?

Finally, the most striking thing about predictions made in 2025 is how many of them converge on a single gravity well: "big things by 2030." You can feel the forecasting community compressing uncertainty into that window. The next few years will decide whether this is a genuine signal—or just synchronized narrative drift.

## Part I — Predictions About 2025 (Now Graded)

### 2023: The "Surely Not Yet" Era
Jessica Taylor offered a cautious take: maybe models wouldn't solve a particular structured word-sequence puzzle, or at least not the exact version posed.
- Verdict: False.
- Lesson: Reasoning-oriented model progress surprised on the upside. Not just bigger autocomplete—something closer to brittle but real problem-solving.

### 2024: The Year of Confident Specificity
- teortaxesTex predicted near "o3 level" models running locally: 256 GB VRAM, Q3 2025, >40 tokens/sec—framed as mostly a question of compute and willpower.
  - Verdict: False, but uncomfortably close. DeepSeek V3.1 fell short of o3 on Artificial Analysis; V3.2 got closer—but slipped to Q4 2025.
- Jack Gallagher called it: by late 2025, we mostly wouldn't use Adam anymore.
  - Verdict: Partially correct. Muon gained real momentum (including use in Kimi K2 and GLM 4.5), but the shift wasn't the clean replacement story implied—and algorithmic iteration didn't explode the way the prediction's tone suggested.
- Elon Musk: "AI will probably be smarter than any single human next year."
  - Verdict: Mostly false. Evaluation is tricky because AI is jagged—superhuman in narrow ways, very human-ish or worse in others—but the strong version doesn't survive contact with reality.
- Aidan McLau: 60% odds that an o-series model solves a Millennium Prize Problem in 2025.
  - Verdict: False. (And a great example of why we should treat math breakthrough timelines as weirdly resistant to extrapolation.)
- Victor Taelin staked money on a crisp definition: "AGI" as proving theorems in a proof assistant as competently as Taelin.
  - Verdict: False. But: This is the kind of definition that makes forecasting honest. Even when wrong, it's useful.

## Part II — Predictions Made in 2025 About 2025 (The Fastest to Grade)
- Gary Marcus predicted that no single system would solve more than four Marcus–Brundage "AI 2027" tasks by end of 2025, and maybe none would be reliably solved.
  - Verdict: Correct. Reading comprehension might be arguable, but the "4 tasks" bar wasn't reached.
- Dario Amodei: within 3–6 months, AI is writing 90% of the code.
  - Verdict: False in the sense that matters. Counting lines is the wrong metric. Autonomy and responsibility-bearing output didn't reach "90% of software development" reality.
- @kimmonismus predicted Manus would soon replace 50% of white-collar jobs.
  - Verdict: False.
- Miles Brundage warned that 2025 would likely see dangerous AI capabilities capable of causing a massive incident.
  - Verdict: Probably false. At minimum there's no strong public evidence matching the scale described.
- @chatgpt21: 75% on "humanity's last exam" by the end of year.
  - Verdict: False. Gemini 3 Pro peaked at 37.2%.

## Part III — The New Batch: Predictions Pointing Forward (2026–2035+)
This is where the forecast landscape changes shape. Once you move beyond a single year, the tone shifts from "here's a benchmark" to "here's a worldview."

### 2026: Agents, Robotics, and the Great Overhang of "Most Code"
A cluster of 2025-made predictions expects very rapid agentic capability gains by 2026:
- Zuckerberg: most code for Meta's agent efforts written by AI within 12–18 months (not autocomplete).
- Julian Schrittwieser: models autonomously working full 8-hour days by mid-2026.
- Mustafa Suleyman: action-taking over arbitrarily long horizons by end of next year.
- Musk / Patterson / Mostaque / Taelin: variations on "AGI this year / next year / more likely than not."

Also: robots enter the chat. Teortaxes (Deepseek) predicts a Chinese company credibly showing hundreds of robots around the Spring Festival Gala (Feb 16, 2026). Gary Marcus predicts domestic humanoid robots remain mostly demo.

These will be unusually easy to evaluate: robots are public-facing, and "full-day autonomy" is hard to fake convincingly.

### 2027–2028: The "Country of Geniuses in a Datacenter" Zone
By 2027 and especially 2028, forecasts thicken into bold statements of near-inevitability:
- Anthropic: powerful systems emerging late 2026/early 2027; Nobel-class cognition across domains; extended autonomy; real-world interfacing.
- Amodei / Legg / Critch / 80k Hours / Sholto Douglas: strong odds of systems able to automate huge swaths of intellectual work by 2027–28.
- Greenblatt / METR: longer reliable autonomy horizons—multi-week projects within the decade, possibly sooner.

Whether these happen hinges less on raw benchmarks and more on deployment reality: reliability, tool use, memory, coordination, and economics.

### 2030: The Shared Magnet
If 2026 is the "agents soon" cluster, 2030 is the cultural convergence point: sweeping job automation claims, math domination forecasts, grid-scale compute predictions, and existential-risk timelines all stack up here.

You get:
- "Replace >50% of humans" claims,
- "Annals-quality math for <$100k inference" bets,
- projections of gigantic training clusters,
- and dueling narratives: bubble-burst skeptics vs. accelerationists vs. "normal technology" gradualists.

It's striking how many mutually incompatible stories all land on the same calendar year.

## What This Pattern Suggests (Without Pretending It's Science)
- Near-term predictors systematically overreach.
- Even when they're "close," the last 10–20% matters a lot: reliability, cost, autonomy, integration, and real-world friction.
- "AGI" is dissolving as an evaluation category.
- The forecasts that age best are the ones that anchor to tasks, constraints, and measurable thresholds.
- Forecasting is becoming synchronized. The 2030 clustering looks like a narrative attractor: a year far enough away to be dramatic, near enough to feel accountable.
- The next big disambiguator is agents in the wild. Not "can it pass exam X," but "can it run a messy, long, adversarial workflow end-to-end without babysitting?"

## A More Honest Way to Track This Going Forward
If I were turning this into a tighter annual scoreboard, I'd grade predictions on:
- Operational clarity: could a neutral observer test it?
- Time horizon: short gets extra weight.
- Economic reality: "possible" isn't "dominant."
- Reliability threshold: median success isn't the same as "replace jobs."
- Constraint realism: tool access, memory, security, cost, latency, oversight.
